# Moderator Handbook - KB.fanz.website

**Category:** Compliance & Legal > Moderation
**Last Updated:** December 2024
**Applies to:** Platform Moderators, Compliance Team
**Legal Requirement:** Yes - Training mandatory for all moderation staff
**Target Audience:** Moderators, Compliance Staff

---

## Overview

This handbook is the comprehensive training manual for all Fanz Dash content moderators. It covers content review procedures, policy enforcement, escalation protocols, CSAM response, trafficking detection, and mental health resources.

**Moderation is critical to platform safety, legal compliance, and user trust.**

---

## Table of Contents

1. [Moderator Role & Responsibilities](#moderator-role--responsibilities)
2. [Content Review Procedures](#content-review-procedures)
3. [Policy Violations Identification](#policy-violations-identification)
4. [Enforcement Actions](#enforcement-actions)
5. [CSAM Response Protocol](#csam-response-protocol)
6. [Trafficking Indicators](#trafficking-indicators)
7. [Escalation Procedures](#escalation-procedures)
8. [User Reports](#user-reports)
9. [Appeals & Disputes](#appeals--disputes)
10. [Tools & Systems](#tools--systems)
11. [Mental Health & Burnout Prevention](#mental-health--burnout-prevention)
12. [Training & Certification](#training--certification)

---

## Moderator Role & Responsibilities

### What Moderators Do

**Primary Responsibilities:**

1. **Review Flagged Content**
   - AI-flagged content requiring human review
   - User-reported content
   - Random sampling for quality assurance

2. **Enforce Platform Policies**
   - Identify policy violations
   - Issue warnings, suspensions, bans
   - Document enforcement actions

3. **Protect Platform from Legal Liability**
   - Detect CSAM (without viewing - use blurred thumbnails only)
   - Identify trafficking indicators
   - Report federal law violations

4. **Support User Safety**
   - Respond to abuse reports
   - Remove harmful content
   - Coordinate with law enforcement when necessary

5. **Maintain Audit Trail**
   - Document all review decisions
   - Provide reasoning for enforcement actions
   - Ensure compliance with GDPR/DSA (EU users)

---

### What Moderators Do NOT Do

**NOT Your Responsibility:**

1. **Direct Law Enforcement Coordination**
   - Handled by legal team
   - You report internally, legal contacts authorities

2. **Customer Support**
   - Technical issues, billing, account access
   - Redirect to support@fanz.com

3. **Content Creation Advice**
   - Not moderators' role to advise on how to create compliant content
   - Redirect to creator resources

4. **Policy Decisions**
   - You enforce existing policies, not create new ones
   - Policy changes come from legal/compliance leadership

---

### Moderator Authority Levels

**Fanz Dash has 3 moderator levels:**

**Level 1: Junior Moderator**
- **Experience:** 0-6 months
- **Authority:**
  - Review low-risk flagged content (AI score 40-59)
  - Issue warnings (first offense)
  - Escalate medium/high-risk content
- **Restrictions:**
  - Cannot issue suspensions or bans
  - Cannot review CSAM-flagged content
  - Cannot handle appeals

**Level 2: Senior Moderator**
- **Experience:** 6+ months, proven track record
- **Authority:**
  - Review all risk levels (40-100)
  - Issue warnings, temporary suspensions (up to 30 days)
  - Review CSAM-flagged content (blurred thumbnails only)
  - Handle Level 1 appeals
- **Restrictions:**
  - Cannot issue permanent bans (except CSAM)
  - Cannot handle high-profile appeals

**Level 3: Lead Moderator / Compliance Officer**
- **Experience:** 12+ months, senior-level expertise
- **Authority:**
  - Review all content, all risk levels
  - Issue permanent bans
  - Coordinate with legal team
  - Handle all appeals
  - Train and mentor junior moderators
  - Make final content decisions

---

## Content Review Procedures

### Content Review Queue

**How Content Enters Review Queue:**

1. **AI Flagged** (50-79 risk score)
   - AI detected potential policy violation
   - Requires human verification
   - Most common source

2. **User Reported**
   - Another user reported content
   - Categorized by report reason
   - Prioritized by severity

3. **Random Sampling** (Quality Assurance)
   - 1% of all uploads randomly reviewed
   - Ensures AI accuracy
   - Identifies policy gaps

4. **Creator Appeal**
   - Creator appeals blocked/removed content
   - Assigned to senior moderator

---

### Review Queue Prioritization

**Priority Levels:**

**P0 - Critical (Review Within 1 Hour)**
- CSAM flagged (AI score ≥80)
- Trafficking indicators (critical risk)
- Imminent harm reports (suicide threats, violence)
- Law enforcement requests

**P1 - High (Review Within 4 Hours)**
- CSAM flagged (AI score 60-79)
- Trafficking indicators (high risk)
- Payment processor violations (risk ≥80)
- User safety reports (harassment, doxxing)

**P2 - Medium (Review Within 24 Hours)**
- Payment processor violations (risk 60-79)
- Trafficking indicators (medium risk)
- Prohibited content (bestiality, incest, etc.)
- Multiple user reports on same content

**P3 - Low (Review Within 72 Hours)**
- Minor policy violations (metadata errors, etc.)
- Low-risk AI flags (40-59)
- Random sampling reviews
- Creator appeals (non-urgent)

---

### Step-by-Step Review Process

**Standard Review Workflow:**

**Step 1: Open Review Ticket**
- System assigns ticket to you
- Displays content metadata, AI scores, user reports
- Shows blurred thumbnail (if image/video)

**Step 2: Analyze Metadata**
- **File Information:** Name, size, dimensions, upload timestamp
- **User Information:** Account age, verification status, prior violations
- **AI Analysis:** Risk scores, detected categories, confidence levels
- **User Reports:** Number of reports, report reasons, reporter credibility

**Step 3: Review Blurred Thumbnail (If Applicable)**
- For images/videos, system shows heavily blurred thumbnail
- Can determine: Body proportions (adult vs. child), sexual vs. non-sexual context, number of people
- Cannot see: Faces, explicit details, identifying information

**Step 4: Check Against Policies**
- Review applicable policies (see [Policy Violations](#policy-violations-identification))
- Determine: Does content violate policy?
- Consider context (is it educational, artistic, etc.?)

**Step 5: Make Decision**
- **Approve:** Content is policy-compliant, release to platform
- **Warning:** Minor violation, first offense - remove content, warn user
- **Suspension:** Serious violation or repeat offense - remove content, suspend account
- **Ban:** Severe violation (CSAM, trafficking) - remove all content, permanent ban
- **Escalate:** Unclear or requires senior review

**Step 6: Document Decision**
- Required fields:
  - Decision (approve/warning/suspension/ban/escalate)
  - Reasoning (specific policy violated or why approved)
  - Evidence (AI scores, user reports, specific content elements)
  - Action taken (content removed, account suspended, etc.)
- Optional: Internal notes for future reference

**Step 7: Notify User (If Action Taken)**
- System automatically sends notification
- For EU users: "Statement of Reasons" (DSA requirement)
- Includes: What was removed, why, appeal rights

**Step 8: Close Ticket**
- Ticket marked complete
- Audit log updated
- Statistics updated (for transparency reporting)

---

### Decision Matrix

**Use this matrix for quick reference:**

| AI Risk Score | User Reports | Prior Violations | Action |
|--------------|--------------|------------------|--------|
| 40-59 | 0-1 | None | Likely **Approve** (false positive) |
| 40-59 | 2-5 | 1-2 warnings | Likely **Warning** |
| 60-79 | 0-1 | None | **Review carefully**, could be false positive |
| 60-79 | 2-5 | 1-2 warnings | Likely **Suspension** (7-14 days) |
| 60-79 | 6+ | 3+ violations | **Suspension** (30 days) or escalate for ban |
| 80-89 | Any | None | **Suspension** (7-30 days) + escalate |
| 80-89 | Any | 1+ violations | **Escalate** for permanent ban |
| 90-100 | Any | Any | **Immediate escalation** (likely CSAM or severe violation) |

**This is guidance only. Use judgment and context.**

---

## Policy Violations Identification

### CSAM (Child Sexual Abuse Material)

**Federal Law Violation - Mandatory Reporting**

**Identification (Without Viewing):**
- AI age estimation: Subject appears under 18
- AI sexual content detection: Sexual content present
- Combined score ≥80: High likelihood of CSAM

**Blurred Thumbnail Indicators:**
- Child-like body proportions (small frame, underdeveloped)
- Sexual context (poses, acts)
- Multiple subjects (potential production)

**Metadata Indicators:**
- File names containing age references ("14yo", "young", "child")
- User account recently created (throwaway account)
- Upload from known high-risk IP ranges

**Action:**
- **NEVER VIEW FULL CONTENT** (federal crime)
- Immediate escalation to Level 3 (Lead Moderator)
- If AI score ≥90: Auto-report to NCMEC, immediate ban (no review needed)
- Document only metadata (never describe content)

**Related:** [CSAM Detection Guide](./csam-detection-guide.md)

---

### Payment Processor Violations

**Critical for Platform Survival - Strict Enforcement**

**9 Prohibited Categories:**

**1. Bestiality**
- Sexual acts with animals
- Any animal in sexual context
- Indicators: Animals visible in sexual content, zoophilia terms

**2. Incest (Real or Roleplay)**
- Family relationships (mother/son, father/daughter, siblings)
- "Step-" relationships (stepmom, stepbrother, etc.)
- Terms: "daddy/daughter", "mommy/son", "family", "brother/sister"
- **Even roleplay is prohibited**

**3. Minors in Sexual Situations**
- Anyone appearing under 18 in sexual content
- "Barely legal", "teen", "18+" in sexual contexts
- School uniforms, children's toys in sexual content
- Young appearance + sexual content

**4. Non-Consensual Content**
- Rape scenarios, even roleplay
- "Forced", "drugged", "sleeping", "unwilling"
- Coercion, blackmail scenarios
- Hidden camera scenarios

**5. Necrophilia**
- Sexual content involving death or corpses
- "Dead", "corpse", death roleplays in sexual contexts

**6. Excrement / Bodily Waste**
- Scat (feces), vomit, extreme bodily functions
- Urination in sexual contexts (depends on platform - OnlyFans prohibits, Fansly allows with warnings)

**7. Extreme Violence**
- Gore, blood, torture in sexual contexts
- Snuff fantasies
- Extreme pain, injury

**8. Trafficking Indicators**
- See [Trafficking Indicators](#trafficking-indicators) section
- Content suggesting coercion, control, exploitation

**9. Age Play (Appearing Minor)**
- Adults roleplaying as children in sexual contexts
- Pacifiers, diapers, "little" language in sexual content
- Emphasis on youthfulness in sexual contexts

**Identification:**
- AI risk score ≥80 for payment processor categories
- Caption/description contains prohibited terms
- Visual content shows prohibited acts
- User reports citing prohibited content

**Action by Risk Score:**
- 80-89: Remove content, issue warning (first offense) or suspension (repeat)
- 90-100: Remove content, escalate for permanent ban

---

### Trafficking Indicators

**Federal Law (FOSTA-SESTA) - Mandatory Reporting if Confirmed**

**13 Indicators to Check:**

**1. Third-Party Management**
- Mentions of "manager", "agency", "boss" controlling content
- Payment going to third party
- Instructions from someone else

**2. Rapid Location Changes**
- Multiple cities in short time period
- Hotel/motel backgrounds (transient)
- "Touring" language (common in trafficking)

**3. Multiple Performers, Single Account**
- One account posts content with different people
- Could indicate one person controlling multiple victims
- Note: Legitimate creator collaborations are okay (look for other indicators)

**4. Pricing Anomalies**
- Suspiciously low prices (desperation for money)
- Sudden price drops
- "Need money urgently" language

**5. Coercion Language**
- "Have to", "forced to", "need money for [debt/someone]"
- Mentions of debt, owing money
- Lack of agency ("my boss makes me", "I have to")

**6. Age Ambiguity**
- Looks very young (even if verified 18+)
- Mentions school, living with parents
- Emphasizes youth ("just turned 18", "fresh")

**7. Restricted Communication**
- Cannot message freely, delayed responses
- Someone else seems to be controlling account
- Requests to communicate only through manager/third party

**8. ID Document Requests**
- Asking subscribers for ID (potential blackmail)
- Requests for personal info (address, workplace)

**9. Cash-Only Payments**
- Avoids traceable payment methods
- Cash app, cryptocurrency only
- Requests for in-person payments

**10. Hotel/Motel Backgrounds**
- Frequently changes locations
- Transient accommodations
- No stable home environment

**11. Branding/Tattoos**
- Tattoos that could indicate ownership ("Property of [name]")
- Branding marks
- Symbols associated with trafficking

**12. Substance Abuse Indicators**
- Appears intoxicated in content
- Visible drug paraphernalia
- Mentions addiction, "need fix"

**13. Distress Signals**
- Appears scared, coerced, uncomfortable
- Eye contact avoidance (when not stylistic)
- Bruises, injuries (not BDSM context)

**Risk Scoring:**
- 0-2 indicators: Low risk (no action)
- 3-5 indicators: Medium risk (monitor, document)
- 6-8 indicators: High risk (escalate for investigation)
- 9+ indicators: Critical (immediate escalation + report)

**Action:**
- High/Critical risk: Escalate immediately to Level 3 + Legal
- Do NOT notify user (could endanger victim)
- Preserve all evidence
- If confirmed: Report to FBI, local law enforcement

---

### Other Policy Violations

**Terms of Service Violations:**

**Spam / Commercial Solicitation**
- Excessive promotional content
- Selling non-approved products/services
- Pyramid schemes, MLM

**Harassment / Abuse**
- Targeted harassment of users
- Hate speech, threats
- Doxxing (sharing personal info)

**Impersonation**
- Pretending to be someone else
- Using someone else's photos without permission
- Celebrity impersonation

**Copyright Infringement**
- Posting copyrighted content without permission
- DMCA takedown requests

**Account Security Violations**
- Account sharing
- Selling account access
- Unauthorized access

**Action:** Typically warnings and suspensions (not permanent bans unless severe or repeated)

---

## Enforcement Actions

### Warning (Level 1)

**When to Issue:**
- First offense
- Minor violation
- Unclear intent (could be accidental)
- Borderline content

**Process:**
1. Remove violating content
2. Send warning email to user
3. Document violation in user record
4. Content remains removed, but account stays active

**Warning Email Includes:**
- What content was removed
- Specific policy violated
- Warning that future violations may result in suspension/ban
- How to avoid violations (link to policies)

**Duration:** Warning stays on record for 90 days. After 90 days with no further violations, warning expires.

---

### Temporary Suspension (Level 2)

**When to Issue:**
- Second or third offense within 90 days
- Serious violation (but not federal law)
- Payment processor risk (80-89 score)
- Multiple minor violations

**Suspension Durations:**
- **7 days:** Second offense, minor-to-moderate violation
- **14 days:** Third offense, moderate violation
- **30 days:** Serious violation, multiple repeat offenses

**Process:**
1. Remove all violating content
2. Make remaining content private (not deleted - may need for evidence)
3. Suspend account (user cannot log in)
4. Send suspension notice with:
   - Reason for suspension (specific policy violated)
   - Suspension duration
   - Content removed
   - Appeal rights (EU users - DSA)
   - What happens if further violations occur
5. Schedule auto-reinstatement (after suspension period)

**During Suspension:**
- User cannot access account
- Payments paused (subscribers not charged)
- Messages held (not delivered)
- User can still appeal via email

**After Suspension:**
- Account automatically reinstated
- User receives email: "Your account has been reinstated. Further violations may result in permanent ban."
- Content remains private (user must re-publish)

---

### Permanent Ban (Level 3)

**When to Issue:**
- CSAM detected
- Confirmed trafficking
- Fourth violation within 12 months
- Severe payment processor violation (90-100 score)
- Real-world harm (doxxing, credible threats, etc.)

**Process:**
1. Remove ALL content (deleted, not hidden)
2. Permanently ban account (cannot be reinstated)
3. Ban user's IP, device fingerprint, payment methods (prevent new account creation)
4. Refund subscribers (if applicable)
5. Preserve evidence (if federal law violation)
6. Send permanent ban notice (except CSAM - law prohibits notification of CSAM reports)
7. If CSAM or trafficking: Notify legal team for law enforcement reporting

**Permanent Ban Notice Includes:**
- Account permanently banned
- Reason (general - "federal law violation", "severe Terms of Service violation")
- No appeal (for CSAM, trafficking)
- EU users: Appeal rights (DSA - but rarely overturned)

**Global Ban Database:**
- User added to industry ban database (shared with other platforms)
- Prevents access to other compliant adult platforms

---

### Escalation to Legal / Law Enforcement

**When to Escalate:**
- CSAM (confirmed or high likelihood)
- Confirmed trafficking
- Credible threats of violence
- Subpoena or warrant received
- Complicated legal questions

**Process:**
1. Do NOT take further action on content (preserve as-is)
2. Email: legal@fanz.com
3. Subject: "URGENT: [CSAM/Trafficking/Legal] Escalation"
4. Include:
   - User ID
   - Content ID(s)
   - Your assessment and reasoning
   - AI scores and evidence
   - Any relevant user reports
5. Legal team responds within 1-24 hours (depending on severity)
6. Follow legal team's instructions

**DO NOT:**
- Contact user (law prohibits notification for CSAM/trafficking)
- Discuss with other moderators (confidentiality)
- Delete evidence (federal crime - obstruction)

---

## CSAM Response Protocol

### Critical Rules

**1. NEVER VIEW SUSPECTED CSAM DIRECTLY**
- Viewing CSAM is a federal crime (18 U.S.C. § 2252)
- 5-20 years imprisonment per violation
- No exceptions, even for moderators

**2. USE BLURRED THUMBNAILS ONLY**
- System generates heavily blurred thumbnails
- Blur level: Cannot see faces, explicit details
- Can determine: Body proportions, context

**3. WHEN IN DOUBT, ESCALATE**
- If you cannot determine from blurred thumbnail and metadata, escalate
- False positives are acceptable
- False negatives (missing CSAM) are federal crimes

**4. NEVER NOTIFY USER**
- Federal law prohibits notification (18 U.S.C. § 2258A)
- Could interfere with investigation

**5. DOCUMENT METADATA ONLY**
- Never describe content details
- Document: User ID, timestamp, AI scores, hash match info

---

### Step-by-Step CSAM Response

**Step 1: Identify CSAM Flag (AI Score ≥50)**
- System alerts you to CSAM-flagged content
- Priority: P0 (score ≥80) or P1 (score 60-79)

**Step 2: Review ONLY Blurred Thumbnail + Metadata**
- Do NOT unblur or view full content
- Check:
  - AI age estimation (appears <18?)
  - AI sexual content detection (sexual content present?)
  - File metadata (suspicious names, EXIF data)
  - User account (new account, suspicious activity?)

**Step 3: Make Determination**

**If AI Score ≥90 (Very High Confidence):**
- Auto-report to NCMEC (no human review needed)
- Immediate permanent ban
- Evidence preserved
- No notification to user
- Legal team automatically notified

**If AI Score 80-89 (High Confidence):**
- Escalate to Level 3 (Lead Moderator)
- Lead reviews (still blurred thumbnail only)
- If confirmed: Report to NCMEC, permanent ban
- If false positive: Approve content, update AI

**If AI Score 60-79 (Medium-High Confidence):**
- Escalate to Level 3
- More likely false positive (young-looking adult)
- If confirmed: Report
- If false positive: Approve

**If AI Score 50-59 (Medium Confidence):**
- Level 2 moderators can review
- Likely false positive
- If any doubt: Escalate

**Step 4: Document (If Escalating or Confirming)**
- Do NOT describe content
- Document:
  - User ID: [user_id]
  - Upload ID: [upload_id]
  - AI Score: [score]
  - Hash Match: [Yes/No]
  - Metadata: [file name, size, timestamp]
  - Your assessment: "Probable CSAM - escalated" or "Likely false positive - approved"

**Step 5: Follow-Up**
- Legal team handles NCMEC report
- Evidence preserved indefinitely
- You move on to next ticket
- **Do NOT think about the content** - use mental health resources if needed

---

### False Positives (Common)

**Common CSAM False Positives:**

1. **Young-Looking Adults (18-22)**
   - Small frame, youthful appearance
   - AI incorrectly estimates age <18
   - Check: Verification records (is user verified 18+?)

2. **Cartoon/Anime Characters**
   - Stylized art with large eyes, small bodies
   - AI misinterprets as children
   - Check: Is it clearly animated/drawn?

3. **Partially Clothed Non-Sexual Content**
   - Teen in bathing suit (non-sexual beach photo)
   - AI detects age <18 + partial nudity = flags
   - Check: Is content actually sexual?

**How to Handle False Positives:**
- Verify user age (check verification records)
- Confirm content is actually legal adult content
- Approve content
- Document as false positive (helps train AI)

---

## Trafficking Indicators

### Identifying Trafficking

**See [Policy Violations - Trafficking](#trafficking-indicators) for full list of 13 indicators.**

**Pattern Recognition:**

**High-Risk Patterns:**
- New account + multiple performers + rapid location changes
- Coercion language + low prices + restricted communication
- Very young appearance + third-party management + hotel backgrounds

**Medium-Risk Patterns:**
- Third-party management + pricing anomalies
- Multiple performers + one account

**Low-Risk (Likely Legitimate):**
- Solo creator + stable location + normal pricing + verified identity

---

### Trafficking Response Protocol

**If 6+ Indicators Detected (High Risk):**

**Step 1: Immediate Escalation**
- Email: legal@fanz.com
- Subject: "URGENT: Suspected Trafficking - User ID [user_id]"
- Include: All 13 indicators, your findings, evidence

**Step 2: Do NOT Notify User**
- Could endanger victim
- Could tip off trafficker
- Federal law prohibits notification during investigation

**Step 3: Preserve Evidence**
- Content preserved (not deleted)
- Account suspended (user cannot access)
- Message logs, payment records, IP logs preserved

**Step 4: Legal Team Coordinates**
- Legal contacts FBI, local law enforcement
- Provides evidence package
- Coordinates investigation
- You are not involved further (unless law enforcement requests interview)

**Step 5: Follow-Up (Days to Weeks Later)**
- Legal may inform you of outcome (rare)
- Typically no feedback (investigation confidential)
- Account remains permanently banned

---

### Supporting Potential Victims

**If User Reaches Out (Rare):**

**Do NOT:**
- Promise to help directly (not your role)
- Give personal contact info
- Offer to meet in person

**DO:**
- Listen and document what they say
- Provide National Human Trafficking Hotline: 1-888-373-7888
- Escalate to legal team immediately
- Reassure: "We're taking this seriously and will connect you with people who can help."

---

## Escalation Procedures

### When to Escalate

**Always Escalate:**
- CSAM (AI score ≥80)
- Trafficking (6+ indicators)
- Credible threats of violence
- Anything you're unsure about

**Consider Escalating:**
- Repeat offender (3+ violations)
- High-profile account (large following, could impact platform reputation)
- Borderline content (could go either way)
- Complicated policy questions

---

### How to Escalate

**Step 1: Create Escalation Ticket**
- Dashboard → Escalations → New Escalation
- Fill in:
  - User ID
  - Content ID
  - Your assessment
  - Why you're escalating
  - Priority level

**Step 2: Notify**
- Email escalations automatically sent to:
  - Lead Moderators (Level 3)
  - Compliance Officers
  - Legal team (if CSAM, trafficking, or legal issue)

**Step 3: Do Not Take Further Action**
- Leave content/account as-is
- Wait for senior staff to review
- Move on to next ticket in queue

**Step 4: Follow-Up**
- Senior staff reviews and makes decision
- You're notified of outcome
- Learn from decision (improves your future reviews)

---

## User Reports

### Triaging User Reports

**User reports are categorized:**

**1. CSAM**
- Highest priority (P0)
- Immediate review
- Treat as CSAM flag (same protocol)

**2. Trafficking / Exploitation**
- High priority (P1)
- Review within 4 hours
- Check for trafficking indicators

**3. Harassment / Abuse**
- High priority (P1)
- Review within 4 hours
- Check for threats, doxxing, hate speech

**4. Policy Violation**
- Medium priority (P2)
- Review within 24 hours
- Check reported content against policies

**5. Spam**
- Low priority (P3)
- Review within 72 hours
- Remove if confirmed spam

**6. Other / Unclear**
- Low priority (P3)
- Review and recategorize

---

### Evaluating Reporter Credibility

**Not all reports are accurate. Check:**

**Reporter History:**
- First report? Likely legitimate
- Multiple reports per day? Possible abuse (weaponizing reports)
- All reports on same user? Possible harassment campaign

**Report Quality:**
- Specific explanation? More credible
- Generic "this is bad"? Less credible
- Evidence provided? More credible

**Content Analysis:**
- Does content actually violate policy?
- Or is reporter offended but content is policy-compliant?

**Common False Reports:**
- Ex-partners reporting out of revenge
- Competitors trying to get creators banned
- Users offended by legal adult content

**Action:**
- If false report: Dismiss, document reporter as potential abuser
- If multiple false reports from same user: Consider suspending reporter's account (abuse of reporting system)

---

## Appeals & Disputes

### Appeal Process

**Who Can Appeal:**
- Users whose content was removed
- Users whose accounts were suspended or banned
- EU users have enhanced appeal rights (DSA)

**Appeals Process:**

**Step 1: User Submits Appeal**
- Email to legal@fanz.com
- Or through Compliance Dashboard → Appeals

**Step 2: Appeal Triaged**
- Junior moderators handle: Low-stakes appeals (single content removal, first warning)
- Senior moderators handle: Suspensions, multiple removals
- Legal team handles: Permanent bans, CSAM, trafficking

**Step 3: Review**
- Different moderator reviews (not same person who made original decision)
- Reviews original decision, evidence, user's appeal reasoning
- Determines: Was original decision correct?

**Step 4: Decision**

**Appeal Granted:**
- Content reinstated
- Warning/suspension removed from record
- User notified with apology

**Appeal Denied:**
- Original decision stands
- User notified with explanation
- EU users: Can escalate to out-of-court dispute resolution (DSA)

**Partial Grant:**
- Some content reinstated, some remains removed
- Suspension reduced (e.g., 30 days → 14 days)

**Step 5: Notify User**
- Response within 48 hours (standard)
- Response within 14 days (EU users - DSA requirement)

---

### DSA Complaints (EU Users)

**Digital Services Act (EU) Requirements:**

**Internal Complaint System:**
- EU users can appeal content moderation decisions
- 14-day response time (mandatory)
- Must provide "Statement of Reasons" for all removals

**Statement of Reasons Includes:**
- What content was removed
- Specific policy violated (exact clause)
- How decision was reached (AI + human review)
- Information on appeal rights
- How to file internal complaint
- Right to out-of-court dispute resolution

**Process:**
1. User files DSA internal complaint
2. Assigned to senior moderator (Level 2+)
3. Full review of decision
4. Response within 14 days
5. If denied, user can escalate to certified dispute resolution body

**Certified Dispute Resolution:**
- Independent third-party reviews case
- Fanz must comply with resolution
- Rare (most appeals resolved internally)

**Related:** [DSA Complaints Guide](./dsa-complaints.md)

---

## Tools & Systems

### Moderator Dashboard

**Access:** https://dash.fanz.website/moderation

**Key Features:**

**1. Review Queue**
- All flagged content requiring review
- Sortable by priority, age, category
- Filters: CSAM, trafficking, payment processor, user reports

**2. User Lookup**
- Search by username, email, user ID
- View full user history: uploads, violations, reports
- Account standing (warnings, suspensions)

**3. Content Analysis**
- Blurred thumbnails
- AI risk scores and detected categories
- Metadata (file info, EXIF data)
- User reports (if any)

**4. Enforcement Actions**
- Issue warnings, suspensions, bans
- Document reasoning
- Send notifications

**5. Appeals**
- View pending appeals
- Review original decision
- Grant or deny appeals

**6. Statistics**
- Your review stats (reviews per day, accuracy, etc.)
- Team stats (total reviews, enforcement actions)
- Platform stats (for transparency reporting)

---

### AI Content Analysis System

**What AI Detects:**

**Age Estimation:**
- Analyzes facial features, body proportions
- Estimates apparent age: Child (<13), Teen (13-17), Adult (18+)
- Confidence score: 0-100%

**Sexual Content Detection:**
- Detects nudity, sexual acts, suggestive content
- Categories: Explicit, suggestive, non-sexual
- Confidence score: 0-100%

**Payment Processor Categories:**
- 9 prohibited categories
- Risk score per category
- Overall risk score: 0-100

**Trafficking Indicators:**
- 13 indicators
- Counts indicators present
- Risk level: Low, medium, high, critical

**Keyword Detection:**
- Scans captions, descriptions, titles
- Flags prohibited terms (incest language, age play, non-consent)

**AI Accuracy:**
- Overall: 85-95% accurate (varies by category)
- False positives: 5-10% (content flagged but actually compliant)
- False negatives: <1% (violations missed by AI)

**Your Role:**
- AI flags content
- You verify AI's decision
- If AI is wrong: Document as false positive (helps train AI)

---

### Evidence Preservation System

**For CSAM, Trafficking, or Legal Investigations:**

**What's Preserved:**
- Original content (encrypted)
- All metadata (file info, EXIF, upload timestamp)
- User information (name, email, IP, payment, device)
- Communication logs (messages, comments)
- Access logs (who viewed what, when)

**Retention:**
- Indefinite (federal law requirement)
- Cannot be deleted
- Available to law enforcement with valid warrant/subpoena

**Access:**
- Restricted to legal team only
- All access logged
- Audit trail maintained

**Your Role:**
- When you escalate CSAM/trafficking, system automatically preserves evidence
- You do not access preserved evidence (legal team only)

---

## Mental Health & Burnout Prevention

### Why Moderator Mental Health Matters

**Content moderation is psychologically harmful:**

- Exposure to disturbing content (even blurred)
- Constant vigilance for worst-case scenarios (CSAM, violence)
- Vicarious trauma (empathy for victims)
- High-stakes decisions (enforcement affects real people)
- Repetitive, emotionally draining work

**Research shows:**
- 70-80% of content moderators experience PTSD symptoms
- High rates of anxiety, depression, insomnia
- Burnout common after 6-12 months
- Without support, moderators leave industry

**Fanz Dash is committed to moderator well-being.**

---

### Mandatory Mental Health Support

**1. Pre-Employment Screening**
- Psychological assessment before hiring
- Ensures you're prepared for this work
- Not everyone is suited for moderation (and that's okay)

**2. Initial Training**
- Trauma-informed moderation practices
- Self-care strategies
- When and how to seek help

**3. Monthly Counseling (Mandatory)**
- All moderators required to attend
- Free, confidential counseling with licensed therapist
- Specializes in occupational trauma
- 1 hour per month, paid time off for appointment

**4. Rotation Policy**
- No moderator reviews CSAM-flagged content >4 hours/week
- Rotate between content categories (don't specialize in most harmful categories)
- Rotate to different team every 3 months (e.g., user reports → spam → policy violations)
- Mandatory 1-week break every quarter (full paid time off)

**5. Peer Support Groups**
- Weekly optional peer support meetings
- Facilitated by therapist
- Share experiences, coping strategies with other moderators
- Confidential space

**6. Employee Assistance Program (EAP)**
- 24/7 crisis support hotline: [EAP Phone]
- Free counseling sessions (up to 12 per year)
- Resources for family members
- Crisis intervention

---

### Self-Care Strategies

**Daily Practices:**

**1. Set Boundaries**
- Take breaks every 50 minutes (10-minute break)
- No moderation during lunch (full disconnect)
- Clock out on time (no overtime unless urgent)

**2. Decompression Routine**
- End-of-day ritual to transition out of work mode
- Examples: Walk, shower, exercise, hobby
- Do NOT discuss work with family (confidentiality + protects them from vicarious trauma)

**3. Mindfulness**
- Breathing exercises between tickets
- Grounding techniques (5-4-3-2-1 method)
- Be present (don't ruminate on disturbing content)

**4. Physical Health**
- Exercise (releases stress, improves mood)
- Healthy diet (stress eating common in moderation)
- Adequate sleep (7-9 hours)

**5. Social Connection**
- Spend time with loved ones
- Engage in non-work activities
- Maintain hobbies and interests

---

### Warning Signs of Burnout

**Emotional:**
- Feeling numb, detached
- Cynicism, loss of empathy
- Irritability, mood swings
- Anxiety, depression

**Physical:**
- Fatigue, exhaustion
- Insomnia or oversleeping
- Headaches, body aches
- Weakened immune system (getting sick often)

**Behavioral:**
- Avoiding work tasks (especially CSAM/trafficking reviews)
- Decreased performance (slower, less accurate)
- Increased errors
- Calling in sick more often
- Substance use (alcohol, drugs to cope)

**Cognitive:**
- Difficulty concentrating
- Forgetfulness
- Intrusive thoughts (can't stop thinking about content)
- Nightmares about work

**If you experience any of these: Reach out immediately.**

---

### Getting Help

**Step 1: Talk to Your Supervisor**
- Managers are trained to recognize burnout
- Can adjust workload, rotate you to different queue
- No judgment - burnout is normal in this field

**Step 2: Contact EAP**
- 24/7 hotline: [EAP Phone]
- Free, confidential counseling
- Crisis intervention if needed

**Step 3: Take Time Off**
- Use your paid time off (PTO)
- Mental health days are valid reasons to call in
- Extended leave available if needed (FMLA)

**Step 4: Consider Transition**
- Not everyone can do this job long-term (and that's okay)
- Fanz offers internal transfers (to non-moderation roles)
- Career counseling available

**You are not weak for struggling. This work is hard. We support you.**

---

### Peer Support

**Moderator Support Slack Channel:**
- #moderator-support (private channel)
- Vent, share experiences, ask questions
- Facilitated by EAP counselor
- Confidential

**Weekly Peer Meetings:**
- Every Friday, 4pm
- Optional but highly recommended
- Therapist-led discussion
- Share coping strategies

**Buddy System:**
- Each new moderator paired with experienced mentor
- Check in weekly
- Safe person to ask questions, share concerns

---

## Training & Certification

### Required Training

**Before Starting Moderation:**

**1. Platform Policies (4 hours)**
- Terms of Service
- Prohibited content categories
- Payment processor policies
- Age verification and 2257 compliance

**2. Legal Compliance (4 hours)**
- Federal laws (CSAM, trafficking, 2257)
- GDPR and DSA (EU users)
- Evidence preservation
- Law enforcement coordination

**3. CSAM Detection (4 hours)**
- Federal law (18 U.S.C. § 2258A)
- PhotoDNA and PDQ technology
- Safe review procedures (blurred thumbnails)
- Escalation protocols

**4. Trafficking Identification (3 hours)**
- 13 trafficking indicators
- Pattern recognition
- Response protocols
- Supporting victims

**5. Moderation Tools (2 hours)**
- Dashboard walkthrough
- Creating tickets, documenting decisions
- Enforcement actions
- Appeals handling

**6. Mental Health & Self-Care (2 hours)**
- Trauma-informed practices
- Self-care strategies
- Warning signs of burnout
- Resources available

**Total: 21 hours (paid training time)**

**Certification:**
- Written exam (80% passing score)
- Practical exam (review 10 sample tickets)
- Certificate issued upon passing

---

### Ongoing Training

**Monthly:**
- Policy updates (30 minutes)
- Case studies (1 hour) - Real cases, how they were handled
- Q&A with legal/compliance team

**Quarterly:**
- Refresher training (4 hours)
- Mental health check-in with counselor
- Performance review with supervisor

**Annually:**
- Full recertification (10 hours)
- Updated legal/policy changes
- Advanced training (specialized topics)

---

### Advancement Path

**Level 1 → Level 2 (6 months minimum)**
- Requirements:
  - 6+ months experience
  - 95%+ accuracy rate
  - No major errors (false negatives for CSAM/trafficking)
  - Completion of advanced training
- Authority increase: Can review higher-risk content, issue suspensions

**Level 2 → Level 3 (12 months minimum)**
- Requirements:
  - 12+ months experience
  - 97%+ accuracy rate
  - Demonstrated leadership (mentoring, training others)
  - Completion of lead moderator training
- Authority increase: Can issue bans, coordinate with legal, handle appeals

**Lead Moderator → Compliance Officer**
- Requirements:
  - 24+ months experience
  - Exceptional performance
  - Policy expertise
- Role: Oversee moderation team, develop policies, coordinate with legal

---

## FAQ for Moderators

### What if I accidentally see CSAM?

**Stop immediately. Do not continue viewing.**

1. Close the content
2. Email csam@fanz.com immediately
3. Contact EAP: [EAP Phone]
4. Take remainder of day off (paid)
5. Mandatory counseling within 48 hours

**You will not be punished.** Accidental exposure happens. We support you.

---

### Can I review content if I know the user personally?

**No. Recuse yourself (conflict of interest).**

- Assign ticket to another moderator
- Document: "Conflict of interest - personal relationship"
- Do not discuss case with the user

---

### What if a user threatens me?

**Take it seriously.**

1. Document threat (screenshot, save messages)
2. Report to supervisor immediately
3. Escalate to legal team (legal@fanz.com)
4. Do NOT respond to user
5. Account will be banned, threat reported to law enforcement

**Your safety is paramount.**

---

### How do I handle pressure from users to reverse decisions?

**Do not engage directly with users about moderation decisions.**

- All appeals go through formal process (email legal@fanz.com)
- You do not discuss decisions with users
- Refer them to appeals process

**If user is harassing you:**
- Report to supervisor
- Block user from contacting you
- Escalate if harassment continues

---

### What if I disagree with a policy?

**You enforce policies, not create them.**

- If you have feedback on policies: Share with supervisor or compliance team
- Policy discussions happen in separate forum (quarterly policy review meetings)
- Personal beliefs do not override policies

**However:**
- If you believe a policy is illegal or unethical: Escalate to legal team immediately
- Whistleblower protections apply

---

## Contact & Support

### Moderation Team

**Supervisor / Lead Moderator:**
- Email: moderation-lead@fanz.com
- Slack: @moderation-lead
- For: Questions, escalations, feedback

**Compliance Team:**
- Email: compliance@fanz.com
- For: Policy questions, legal issues

**Legal Team:**
- Email: legal@fanz.com
- Phone: [Legal Hotline - 24/7]
- For: CSAM, trafficking, law enforcement, appeals

---

### Mental Health Resources

**Employee Assistance Program (EAP):**
- Phone: [EAP Hotline - 24/7]
- Online: [EAP Portal]
- For: Counseling, crisis support, referrals

**Moderator Support:**
- Slack: #moderator-support
- Weekly meetings: Fridays, 4pm
- Peer support, therapist-facilitated

**National Crisis Hotlines:**
- National Suicide Prevention Lifeline: 988
- Crisis Text Line: Text HOME to 741741
- SAMHSA National Helpline: 1-800-662-4357

---

## Related Articles

### Compliance & Legal
- [Compliance Systems Overview](./compliance-overview.md)
- [CSAM Detection Guide](./csam-detection-guide.md)
- [2257 Compliance](./2257-compliance.md)
- [Prohibited Content List](./prohibited-content.md)

### User Rights
- [GDPR User Rights](./gdpr-user-rights.md)
- [DSA Complaints](./dsa-complaints.md)
- [Troubleshooting Compliance Issues](./troubleshooting-compliance.md)

### Operations
- [API Keys Setup](./api-keys-setup.md)
- [Incident Response Procedures](./incident-response.md)

---

**Last Updated:** December 23, 2024
**Version:** 1.0
**Maintained by:** Fanz Legal & Compliance Team
**Next Review:** February 2025

---

**Confidentiality Notice:** This handbook contains confidential internal procedures and must not be shared outside the moderation team. Disclosure of moderation practices, evidence preservation methods, or specific cases may compromise platform security, user privacy, and law enforcement investigations.

**Mental Health Reminder:** If you are struggling with the emotional demands of this work, please reach out. Your well-being is more important than any ticket. We are here to support you.

**Legal Reminder:** NEVER view suspected CSAM directly. Use blurred thumbnails only. Viewing CSAM is a federal crime. When in doubt, escalate immediately.
